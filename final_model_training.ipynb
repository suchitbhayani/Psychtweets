{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2387b59a-4e0d-42df-9510-7af4d8ad7c43",
   "metadata": {},
   "source": [
    "We will use a RNN this time and see if that helps.\n",
    "\n",
    "https://www.youtube.com/watch?v=AsNTP8Kwu80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67bad9b9-14cf-44b7-b172-43da7fe7f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizerFast, BertModel, get_scheduler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a3d31-93e5-41c2-9e66-b0aef72a288e",
   "metadata": {},
   "source": [
    "# Further Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbfeb6a-8798-48df-98b5-e00a6d51882f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the pope is infallible, this is a catholic dog...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>being you makes you look cute on, because then...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm like entp but idiotichey boy, do you want ...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>give it to ... he has pica since childhood say...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frances farmer will have her revenge on seattl...</td>\n",
       "      <td>intj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7232</th>\n",
       "      <td>god,,pls take care hiro emergency room???? are...</td>\n",
       "      <td>intp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7233</th>\n",
       "      <td>wow last time i got intp i think u upset the f...</td>\n",
       "      <td>intp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7234</th>\n",
       "      <td>a 100% that someone will get his ass kicked so...</td>\n",
       "      <td>entp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7235</th>\n",
       "      <td>if you’re #intj this one is for you | what is ...</td>\n",
       "      <td>infj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7236</th>\n",
       "      <td>hey can you dm me a pic of what harry is weari...</td>\n",
       "      <td>istp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7237 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cleaned text label\n",
       "0     the pope is infallible, this is a catholic dog...  intj\n",
       "1     being you makes you look cute on, because then...  intj\n",
       "2     i'm like entp but idiotichey boy, do you want ...  intj\n",
       "3     give it to ... he has pica since childhood say...  intj\n",
       "4     frances farmer will have her revenge on seattl...  intj\n",
       "...                                                 ...   ...\n",
       "7232  god,,pls take care hiro emergency room???? are...  intp\n",
       "7233  wow last time i got intp i think u upset the f...  intp\n",
       "7234  a 100% that someone will get his ass kicked so...  entp\n",
       "7235  if you’re #intj this one is for you | what is ...  infj\n",
       "7236  hey can you dm me a pic of what harry is weari...  istp\n",
       "\n",
       "[7237 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/cleaned.csv').drop(columns=['Unnamed: 0'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268f2ad2-dc47-4a19-aaaa-ccb1dfb31736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the pope is infallible, this is a catholic dog...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>being you makes you look cute on, because then...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm like entp but idiotichey boy, do you want ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>give it to ... he has pica since childhood say...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>frances farmer will have her revenge on seattl...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7232</th>\n",
       "      <td>god,,pls take care hiro emergency room???? are...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7233</th>\n",
       "      <td>wow last time i got intp i think u upset the f...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7234</th>\n",
       "      <td>a 100% that someone will get his ass kicked so...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7235</th>\n",
       "      <td>if you’re #intj this one is for you | what is ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7236</th>\n",
       "      <td>hey can you dm me a pic of what harry is weari...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7237 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cleaned text  label\n",
       "0     the pope is infallible, this is a catholic dog...     10\n",
       "1     being you makes you look cute on, because then...     10\n",
       "2     i'm like entp but idiotichey boy, do you want ...     10\n",
       "3     give it to ... he has pica since childhood say...     10\n",
       "4     frances farmer will have her revenge on seattl...     10\n",
       "...                                                 ...    ...\n",
       "7232  god,,pls take care hiro emergency room???? are...     11\n",
       "7233  wow last time i got intp i think u upset the f...     11\n",
       "7234  a 100% that someone will get his ass kicked so...      3\n",
       "7235  if you’re #intj this one is for you | what is ...      8\n",
       "7236  hey can you dm me a pic of what harry is weari...     15\n",
       "\n",
       "[7237 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "data['label'] = le.fit_transform(data['label'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f7b46c-1ebb-4435-bb6f-3b9b282a5cc0",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882ce399-cf58-4aed-92c2-cc1164397657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBTIDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    # returns a sample from dataset based on given idx\n",
    "    def __getitem__(self, idx):\n",
    "        item_dct = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item_dct['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fcc1af-0f19-40b6-80e3-7f07bdce5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, train_labels, test_labels = train_test_split(data['cleaned text'].tolist(), data['label'].tolist(), test_size=0.2, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee793c37-1343-4128-b76f-43cea38d35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(text=train_text, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(text=test_text, truncation=True, padding=True)\n",
    "\n",
    "train_data = MBTIDataset(train_encodings, train_labels)\n",
    "test_data = MBTIDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d297b7cd-6783-4773-bbe7-00ce25a8b10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  7098,  1997,  2017,  2035,  1045,  3984, 12399,  2063,  2038,\n",
       "          2025,  2042, 17060,  3689,  2005,  1037,  3232,  1997,  2420,  1010,\n",
       "          2024,  2017,  4364,  3110,  7929,  1029,  7929, 22708,   999,  2054,\n",
       "          2006,  3011,  2052,  2191,  2017,  2228,  2008,  1012,  1012,  1012,\n",
       "          1052,  4246,  2102,  1012,  1012,  1012,  2065,  1045,  2123,  1521,\n",
       "          1056,  5256,  2039,  2000,  2070,  3492, 15281,  1999,  2026,  2606,\n",
       "          2059,  1045,  8415,  2000,  2643,  1012,  1012,  1012,  2065,  2017,\n",
       "          2123,  1521,  1056,  3191,  2026, 12385,  2094,  2077,  2206,  2033,\n",
       "          2008,  1521,  1055,  1037,  2017,  3291,  1012,  2298,  2012,  2017,\n",
       "         11065,  2026,  4485,  2525,  1010,  2057,  1521,  2128,  5306,  2085,\n",
       "         22747,  2860, 18411,  2015,  4429,  4658,  1010, 24978,  2546,  2860,\n",
       "          2025,  4658,  3597,  4747,  1051,  5358,  2546,  2904,  2000,  1037,\n",
       "         24978,  2546,  2860,  1006,  1029,  1007, 18411,  2015,  4429,  1998,\n",
       "          1045,  2079,  2025,  2113,  2054,  2000,  2079,  6761, 12399,  2154,\n",
       "          1998,  2145,  2053,  3492,  1055,  1013,  1051,  5128, 23677, 15281,\n",
       "          1999,  2026,  2606,   999,   999,  4735,  1010,  7078,  4735,  1012,\n",
       "         20228,  2015,  2123,  1521,  1056,  1045,  1521,  1049,  6933,  1031,\n",
       "         23624, 14289, 26255,  2135,  1033, 10166, 10021,  8285, 27108,  2890,\n",
       "          6593,  1045,  3214,  2469,  1045,  2215,  1057,  2025,  2065,  1045,\n",
       "          4139,  1037,  2502, 16584,  1998, 14308,  1037, 21864, 15952,  6293,\n",
       "          2121,  2006,  2009, 13273,  4950,  1998,  2636,  1012,  2085,  2023,\n",
       "          2003,  2183,  2006,  7858,  1000,  2052,  2017,  6235,  4426,  2004,\n",
       "          1037, 22902,  2711,  1029,  1000,  1011,  1045,  6719,  2031,  1037,\n",
       "          2433,  1997,  3638,  3279,  2061,  2748,  2006,  2129,  2000,  2113,\n",
       "          2619,  2003,  2055,  2000,  5745,  1057,  1000,  1011,  1045,  2572,\n",
       "          1996,  5745,  2121, 17111,  9541,  2045,  2242,  1057,  9038,  2013,\n",
       "          3025,  5246,  1029,  2339,  1029,  1000,  1011,  7782,  1997,  2017,\n",
       "          2000,  7868,  3087,  2038,  2412,  2359,  2033,  3793,  2052,  1057,\n",
       "          2066,  2000,  2131,  2157,  2085,  1029,  1000,  1011,  3904,  1045,\n",
       "          2572,  2019,  4372, 22540,  2040, 16424,  2111, 19394,  2052,  1057,\n",
       "          2066,  2000,  2963,  1029,  1000,  1011, 10166,  4830, 20518,  2072,\n",
       "          2017,  1521,  2128,  2061,  2980,  1998,  7916, 10166,  4372, 10536,\n",
       "         11837,  1521,  1055,  9078,  1045,  1521,  1049,  6171,  1004, 23713,\n",
       "          1025, 29179,  1999,  1529,  2017,  2156,  4402,  1010,  1045,  2052,\n",
       "          2021,  2027, 24829,  1521,  3968,  2033,  2043,  1045,  2699,  2000,\n",
       "          2022,  2037,  2767,  2061,  2027,  2079,  2025, 10107,  2026,  2293,\n",
       "          1998, 12242,  4902,  1004, 14181,  1025,  1024,  1006,  1996,  2069,\n",
       "          4489,  2003,  2111,  2097,  2941,  3335,  2017,  2017,  4364,  2876,\n",
       "          1521,  1056,  1029,  1045,  1521,  1049,  2061,  5580,  1010,  2125,\n",
       "          1045,  3769,   999,  1013,  1044,  3501,  2123,  1521,  1056,  2681,\n",
       "          2033,  2182,  2007,  2122,  6881,  2891,  1010,  2202,  2033,  2007,\n",
       "          2017,  8840, 19968,  5180,  4429, 14406,  2172,  2052,  2017,  4364,\n",
       "          5223,  2033,  2065,  1045,  5419, 29300,  2005,  2066,  1037,  3204,\n",
       "          2012,  2087, 11631,  1045,  1521,  1049,  4142, 18411,  2860,  1000,\n",
       "          2054,  2079,  1057,  2424,  3376,  2055,  2166,  1029,  1000,  1011,\n",
       "          1996,  2112,  2073,  2009,  4515,  2001,  1996,  2197, 19394,  2008,\n",
       "          1057,  2288,  1029,  1000,  1011,  1045,  6524,  2131, 19394,  2015,\n",
       "          2122,  2420,  7135,  2021,  2025,  4527,  2072,  2074,  2228,  2008,\n",
       "          2273,  2113,  2054,  1045,  1521,  1049,  3974,  5514,  1010,  2769,\n",
       "          1999,  2026,  2924,  4070,  1010,  1996,  2097,  2000,  2444,  2030,\n",
       "          2026, 10474,  8771,  1012,  2017,  4364,  2342,  2000,  2644,  5128,\n",
       "          2008,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3118c6-0e45-4bc3-afb9-08d9eb2ca475",
   "metadata": {},
   "source": [
    "# Building the Neural Network\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cde3d897-eeea-41c1-a842-72541be27a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBTIPredictor(nn.Module):\n",
    "    def __init__(self, hidden_layer_size, sequence_length):\n",
    "        super(MBTIPredictor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.rnn = nn.RNN(768, hidden_layer_size, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(hidden_layer_size * sequence_length, 16)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        rnn_out, _ = self.rnn(outputs.last_hidden_state)\n",
    "        rnn_out = rnn_out.reshape(rnn_out.size(0), -1)  # Flatten the output\n",
    "        relu_out = self.relu(rnn_out)\n",
    "        logits = self.linear(relu_out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f288e22-dc47-4f5e-80aa-07a97e8299ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1008b7a-626a-45a1-88b8-d7fe67a855ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBTIPredictor(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (rnn): RNN(768, 64, batch_first=True)\n",
      "  (relu): ReLU()\n",
      "  (fc): Linear(in_features=32000, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_size = 64\n",
    "sequence_length = 500\n",
    "\n",
    "model = MBTIPredictor(hidden_layer_size, sequence_length).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf65805-cb79-4786-a109-a65aa04ad27f",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17c911f1-ac0c-4075-a2d5-9a8a8465c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff06eb1e-4c32-4cb7-ae9a-bef7a5022ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader):\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        input_ids = batch_data['input_ids'].to(device)\n",
    "        attention_mask = batch_data['attention_mask'].to(device)\n",
    "        labels = batch_data['labels'].to(device)\n",
    "\n",
    "        pred = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            loss_value = loss.item()\n",
    "            current = batch * len(input_ids)\n",
    "            print(f\"loss: {loss_value:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a63cd-a112-4043-a2aa-8b40428f7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBTIPredictor().to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer, device)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca643366-5955-4438-939f-2a8b56defaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in dataloader:\n",
    "            input_ids = batch_data['input_ids'].to(device)\n",
    "            attention_mask = batch_data['attention_mask'].to(device)\n",
    "            labels = batch_data['labels'].to(device)\n",
    "\n",
    "            model_output = model(input_ids, attention_mask)\n",
    "\n",
    "            _, predicted = torch.max(model_output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a4146-fa12-4c9e-a8be-e068d5fa6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = find_accuracy(model, test_loader)\n",
    "print(f'Model accuracy: {accuracy}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
